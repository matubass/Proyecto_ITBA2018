{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento de Autos y Daño en imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proyecto para procesar y clasificar las imágenes subidas por los usuarios de La Segunda por medio de la app del celular para inspecciones previas.\n",
    "\n",
    "El proyecto persigue dos objetivos:\n",
    "* Detectar si la imágen procesada corresponde a un auto o no\n",
    "* Detectar si el auto de la imágen se encuentra dañado o no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camino recorrido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Búsqueda de datasets de vehículos sanos y dañados por choques. Se armó un dataset compaginado entre datasets de internet, datos reales obtenidos de la empresa en la cual trabajamos e imágenes seleccionadas de forma manual de la búsqueda de google.\n",
    "* Evaluación de alternativas de solución. En función de las consultas realizadas a los profesores y de trabajos relacionados encontrados en internet, decidimos utilizar transfer learning con VGG16.\n",
    "\n",
    "![title](02-guide-how-transfer-learning-v3-06.png)\n",
    "\n",
    "* Se optó por crear una red diferente para cada objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideraciones comunes a ambas redes\n",
    "\n",
    "1. Se utiliza transfer learning desde VGG16\n",
    "    ![title](vgg16.png)\n",
    "2. Las imágenes se redimensionan a un tamaño de 224x224\n",
    "3. Se hace un reescalado de las imágenes de 1/255\n",
    "4. Se aplica data augmentation aleatorio de las siguientes maneras\n",
    "     - Rotacion de 90 grados\n",
    "     - Flip horizontal\n",
    "     - Flip vertical\n",
    "     - Zoom de 0.2\n",
    "5. La red 1 se entrenó con aproximadamente 1500 imágenes por clase. La red 2 se entrenó con aproximadamente 800 imágenes por clase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 1 - Clasificación de auto o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "# dimensión de las imágenes.\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "top_model_weights_path = 'bottleneck_model_2_clases.h5'  \n",
    "train_data_dir = 'C:/data/car-damage/2clases/training'\n",
    "validation_data_dir = 'C:/data/car-damage/2clases/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bottlebeck_features():\n",
    "    \n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                shear_range=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                vertical_flip=True,\n",
    "                                rotation_range=90)\n",
    "\n",
    "    generator = datagen.flow_from_directory(train_data_dir,  \n",
    "                                             target_size=(img_width, img_height),  \n",
    "                                             batch_size=batch_size,  \n",
    "                                             class_mode=None,  \n",
    "                                             shuffle=False)\n",
    "\n",
    "    nb_train_samples = len(generator.filenames)\n",
    "    num_classes = len(generator.class_indices)\n",
    "\n",
    "    predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
    "    bottleneck_features_train = model.predict_generator(generator, predict_size_train)\n",
    "    np.save('bottleneck_2_clases_features_train.npy', bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(validation_data_dir,  \n",
    "                                            target_size=(img_width, img_height),  \n",
    "                                            batch_size=batch_size,  \n",
    "                                            class_mode=None,  \n",
    "                                            shuffle=False)  \n",
    "\n",
    "    nb_validation_samples = len(generator.filenames)\n",
    "\n",
    "    predict_size_validation = int(math.ceil(nb_validation_samples / batch_size))\n",
    "\n",
    "    bottleneck_features_validation = model.predict_generator(generator, predict_size_validation)\n",
    "\n",
    "    np.save('bottleneck_2_clases_features_validation.npy', bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_model(number_model):\n",
    "    datagen_top = ImageDataGenerator(rescale=1./255,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True,\n",
    "                                    vertical_flip=True,\n",
    "                                    rotation_range=90)\n",
    "\n",
    "    generator_top = datagen_top.flow_from_directory(train_data_dir,  \n",
    "                                                    target_size=(img_width, img_height),  \n",
    "                                                    batch_size=batch_size,  \n",
    "                                                    class_mode='categorical',  \n",
    "                                                    shuffle=False)  \n",
    "\n",
    "    nb_train_samples = len(generator_top.filenames)  \n",
    "    num_classes = len(generator_top.class_indices)  \n",
    "\n",
    "    # load the bottleneck features saved earlier  \n",
    "    train_data = np.load('bottleneck_2_clases_features_train.npy')  \n",
    "\n",
    "    # get the class lebels for the training data, in the original order  \n",
    "    train_labels = generator_top.classes  \n",
    "\n",
    "    # convert the training labels to categorical vectors  \n",
    "    train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "    \n",
    "    generator_top = datagen_top.flow_from_directory(validation_data_dir,  \n",
    "                                                target_size=(img_width, img_height),  \n",
    "                                                batch_size=batch_size,  \n",
    "                                                class_mode=None,  \n",
    "                                                shuffle=False)  \n",
    "\n",
    "    nb_validation_samples = len(generator_top.filenames)\n",
    "\n",
    "    validation_data = np.load('bottleneck_2_clases_features_validation.npy')\n",
    "\n",
    "    validation_labels = generator_top.classes\n",
    "    validation_labels = to_categorical(validation_labels, num_classes=num_classes)\n",
    "    \n",
    "    model = prepare_model(number_model, train_data.shape[1:])\n",
    "\n",
    "    checkpointer = ModelCheckpoint(monitor='val_acc', filepath='bottleneck_model_2_clases.h5', verbose=1, save_best_only=True)\n",
    "    \n",
    "    history = model.fit(train_data, train_labels,  \n",
    "                      epochs=epochs,  \n",
    "                      batch_size=batch_size,  \n",
    "                      validation_data=(validation_data, validation_labels),\n",
    "                    callbacks=[checkpointer])  \n",
    "\n",
    "    #model.save_weights(top_model_weights_path)  \n",
    "\n",
    "    (eval_loss, eval_accuracy) = model.evaluate(validation_data, validation_labels, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))  \n",
    "    print(\"[INFO] Loss: {}\".format(eval_loss))\n",
    "    \n",
    "    plt.figure(1)  \n",
    "\n",
    "    # summarize history for accuracy  \n",
    "\n",
    "    plt.subplot(211)  \n",
    "    plt.plot(history.history['acc'])  \n",
    "    plt.plot(history.history['val_acc'])  \n",
    "    plt.title('model accuracy')  \n",
    "    plt.ylabel('accuracy')  \n",
    "    plt.xlabel('epoch')  \n",
    "    plt.legend(['train', 'test'], loc='upper left')  \n",
    "\n",
    "    # summarize history for loss  \n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(number_model, shape):\n",
    "    \n",
    "    if (number_model == 1):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu'))  \n",
    "        model.add(Dropout(0.5))  \n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    elif (number_model == 2):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    elif (number_model == 3):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    elif (number_model == 4):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "        model.compile(optimizer=optimizers.RMSprop(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    elif (number_model == 5):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.8))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.8))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.8))\n",
    "        model.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    elif (number_model == 6):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.8))\n",
    "        model.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    elif (number_model == 7):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    elif (number_model == 8):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    elif (number_model == 9):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    elif (number_model == 10):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(100, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    elif (number_model == 11):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu'))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    elif (number_model == 12):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=optimizers.RMSprop(lr=0.01, decay=0.1), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    elif (number_model == 13):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=optimizers.SGD(lr=0.001, decay=0.1, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    elif (number_model == 14):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=optimizers.RMSprop(lr=0.001, decay=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    elif (number_model == 15):\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de los features por medio de la red VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "save_bottlebeck_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de los Top Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "epochs = 600\n",
    "num_classes = 2\n",
    "train_top_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1200\n",
    "train_top_model(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "train_top_model(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 600\n",
    "train_top_model(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados de cada modelo\n",
    "\n",
    "Los resultados de cada modelo se encuentran detallados en este [link](https://docs.google.com/spreadsheets/d/15_S33peWDFdnzXx5T0yxGEaGsqcYP6dQLyk6DUqXo5A/edit#gid=0) en la solapa \"Auto o No\".\n",
    "\n",
    "##### El modelo seleccionado como el mejor es el 8\n",
    "- Acc: 0,9922\n",
    "- Val Acc: 0,9222\n",
    "- Loss: 1,6056\n",
    "- Val Loss: 1,7623"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "\n",
    "top_model_weights_path = 'bottleneck_model_2_clases.h5'\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "datagen_train_fine = ImageDataGenerator(rescale=1./255,\n",
    "                                shear_range=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                vertical_flip=True,\n",
    "                                rotation_range=90)\n",
    "\n",
    "generator_train_fine = datagen_train_fine.flow_from_directory(train_data_dir,  \n",
    "                                                target_size=(img_width, img_height),  \n",
    "                                                batch_size=batch_size,  \n",
    "                                                class_mode='categorical',  \n",
    "                                                shuffle=False)\n",
    "\n",
    "nb_train_samples_fine = len(generator_train_fine.filenames)  \n",
    "num_classes = len(generator_train_fine.class_indices)  \n",
    "\n",
    "train_data = generator_train_fine\n",
    "train_labels = generator_train_fine.classes  \n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "\n",
    "generator_validation_fine = datagen_train_fine.flow_from_directory(validation_data_dir,  \n",
    "                                                        target_size=(img_width, img_height),  \n",
    "                                                        batch_size=batch_size,  \n",
    "                                                        class_mode='categorical',  \n",
    "                                                        shuffle=False)  \n",
    "\n",
    "nb_validation_samples_fine = len(generator_validation_fine.filenames)\n",
    "\n",
    "validation_data = nb_validation_samples_fine\n",
    "validation_labels = generator_validation_fine.classes\n",
    "validation_labels = to_categorical(validation_labels, num_classes=num_classes)\n",
    "\n",
    "\n",
    "base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height,3))\n",
    "\n",
    "print(base_model.output_shape[1:])\n",
    "\n",
    "# El top model con mejor resultado\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))  \n",
    "top_model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "top_model.compile(optimizer=optimizers.Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "model_fine = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "\n",
    "#Seteo como no entrenables las primeras capas de la red\n",
    "for layer in model_fine.layers[:15]:\n",
    "    print(layer.name)\n",
    "    layer.trainable = False\n",
    "model_fine.layers[15].name\n",
    "\n",
    "model_fine.compile(optimizer=optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model_fine.summary())\n",
    "\n",
    "#model_fine = load_model('MRETZLAF_2CLASES_FINE.hdf5')\n",
    "\n",
    "checkpointer = ModelCheckpoint(monitor='val_acc', filepath='MRETZLAF_2CLASES_FINE.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "history_fine = model_fine.fit_generator(generator_train_fine,\n",
    "                                        steps_per_epoch=nb_train_samples_fine // batch_size,\n",
    "                                        epochs=epochs,\n",
    "                                        validation_data=generator_validation_fine,\n",
    "                                        validation_steps=nb_validation_samples_fine // batch_size,\n",
    "                                        verbose=1,\n",
    "                                        callbacks=[checkpointer])\n",
    "\n",
    "plt.plot(history_fine.history['loss'])\n",
    "plt.plot(history_fine.history['val_loss'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_fine.history['acc'])\n",
    "plt.plot(history_fine.history['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "\n",
    "**Accuracy**\n",
    "- Acc: 0,9974\n",
    "- Val Acc: 0,96667\n",
    "![Accuracy](Fine_acc_red1.png)\n",
    "\n",
    "**Loss**\n",
    "- Loss: 0,0147\n",
    "- Val Loss: 0,2033\n",
    "![title](Fine_loss_red1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = 'C:/data/car-damage/2clases/testing'\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "flow_test = test_datagen.flow_from_directory(test_data_dir,  target_size=(img_width, img_height),  batch_size=batch_size,class_mode='categorical')\n",
    "print(model_fine.metrics_names)\n",
    "model_fine.evaluate_generator(flow_test,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test Acc: 0,953135\n",
    "- Test Loss: 0,2075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 2 - Auto dañado o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "#Ruta del dataset\n",
    "location = 'C:/data/car-damage/damaged_or_not'\n",
    "\n",
    "#Dimensiones de la imagen\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "train_data_dir = location+'/training'\n",
    "validation_data_dir = location+'/validation'\n",
    "\n",
    "# number of epochs to train top model  \n",
    "epochs = 50\n",
    "# batch size used by flow_from_directory and predict_generator  \n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bottlebeck_features():\n",
    "    \n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                shear_range=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                vertical_flip=True,\n",
    "                                rotation_range=90)\n",
    "\n",
    "    generator = datagen.flow_from_directory(train_data_dir,  \n",
    "                                             target_size=(img_width, img_height),  \n",
    "                                             batch_size=batch_size,  \n",
    "                                             class_mode=None,  \n",
    "                                             shuffle=False)\n",
    "\n",
    "    nb_train_samples = len(generator.filenames)\n",
    "    num_classes = len(generator.class_indices)\n",
    "\n",
    "    predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
    "    bottleneck_features_train = model.predict_generator(generator, predict_size_train)\n",
    "    np.save('bottleneck_damaged_or_not_features_train.npy', bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(validation_data_dir,  \n",
    "                                            target_size=(img_width, img_height),  \n",
    "                                            batch_size=batch_size,  \n",
    "                                            class_mode=None,  \n",
    "                                            shuffle=False)  \n",
    "\n",
    "    nb_validation_samples = len(generator.filenames)\n",
    "\n",
    "    predict_size_validation = int(math.ceil(nb_validation_samples / batch_size))\n",
    "\n",
    "    bottleneck_features_validation = model.predict_generator(generator, predict_size_validation)\n",
    "\n",
    "    np.save('bottleneck_damaged_or_not_features_validation.npy', bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_model(number_model):\n",
    "    datagen_top = ImageDataGenerator(rescale=1./255,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True,\n",
    "                                    vertical_flip=True,\n",
    "                                    rotation_range=90)\n",
    "\n",
    "    generator_top = datagen_top.flow_from_directory(train_data_dir,  \n",
    "                                                    target_size=(img_width, img_height),  \n",
    "                                                    batch_size=batch_size,  \n",
    "                                                    class_mode='categorical',  \n",
    "                                                    shuffle=False)  \n",
    "\n",
    "    nb_train_samples = len(generator_top.filenames)  \n",
    "    num_classes = len(generator_top.class_indices)  \n",
    "\n",
    "    # load the bottleneck features saved earlier  \n",
    "    train_data = np.load('bottleneck_damaged_or_not_features_train.npy')  \n",
    "\n",
    "    # get the class lebels for the training data, in the original order  \n",
    "    train_labels = generator_top.classes  \n",
    "\n",
    "    # convert the training labels to categorical vectors  \n",
    "    train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "    \n",
    "    generator_top = datagen_top.flow_from_directory(validation_data_dir,  \n",
    "                                                target_size=(img_width, img_height),  \n",
    "                                                batch_size=batch_size,  \n",
    "                                                class_mode=None,  \n",
    "                                                shuffle=False)  \n",
    "\n",
    "    nb_validation_samples = len(generator_top.filenames)\n",
    "\n",
    "    validation_data = np.load('bottleneck_damaged_or_not_features_validation.npy')\n",
    "\n",
    "    validation_labels = generator_top.classes\n",
    "    validation_labels = to_categorical(validation_labels, num_classes=num_classes)\n",
    "    \n",
    "    model = prepare_model(number_model, train_data.shape[1:])\n",
    "\n",
    "    checkpointer = ModelCheckpoint(monitor='val_acc', filepath='bottleneck_damaged_or_not.h5', verbose=1, save_best_only=True)\n",
    "    \n",
    "    history = model.fit(train_data, train_labels,  \n",
    "                      epochs=epochs,  \n",
    "                      batch_size=batch_size,  \n",
    "                      validation_data=(validation_data, validation_labels),\n",
    "                    callbacks=[checkpointer])  \n",
    "\n",
    "    #model.save_weights(top_model_weights_path)  \n",
    "\n",
    "    (eval_loss, eval_accuracy) = model.evaluate(validation_data, validation_labels, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    #print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))  \n",
    "    #print(\"[INFO] Loss: {}\".format(eval_loss))\n",
    "    \n",
    "    plt.figure(1)  \n",
    "\n",
    "    # summarize history for accuracy  \n",
    "\n",
    "    plt.subplot(211)  \n",
    "    plt.plot(history.history['acc'])  \n",
    "    plt.plot(history.history['val_acc'])  \n",
    "    plt.title('model accuracy')  \n",
    "    plt.ylabel('accuracy')  \n",
    "    plt.xlabel('epoch')  \n",
    "    plt.legend(['train', 'test'], loc='upper left')  \n",
    "\n",
    "    # summarize history for loss  \n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(number_model, shape):\n",
    "    \n",
    "    if (number_model == 1):\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    elif (number_model == 2):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    elif (number_model == 3):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    elif (number_model == 4):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    elif (number_model == 5):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    elif (number_model == 6):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    elif (number_model == 7):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    elif (number_model == 8):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=optimizers.RMSprop(lr=0.01, decay=0.1), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    elif (number_model == 9):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu'))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(500, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "        model.compile(optimizer=optimizers.RMSprop(lr=0.01, decay=0.1), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    elif (number_model == 10):\n",
    "        model = Sequential()  \n",
    "        model.add(Flatten(input_shape=shape))  \n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=optimizers.RMSprop(lr=0.01, decay=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de los features por medio de la red VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "save_bottlebeck_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de los Top Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "epochs = 300\n",
    "num_classes = 2\n",
    "train_top_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "train_top_model(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1500\n",
    "train_top_model(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "train_top_model(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados de cada modelo\n",
    "\n",
    "Los resultados de cada modelo se encuentran detallados en este [link](https://docs.google.com/spreadsheets/d/15_S33peWDFdnzXx5T0yxGEaGsqcYP6dQLyk6DUqXo5A/edit#gid=0) en la solapa \"Dañado o Sano\".\n",
    "\n",
    "##### El modelo seleccionado como el mejor es el 8\n",
    "- Acc: 0,8989\n",
    "- Val Acc: 0,8743\n",
    "- Loss: 0,3245\n",
    "- Val Loss: 0,4068"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "\n",
    "top_model_weights_path = 'bottleneck_damaged_or_not.h5'\n",
    "epochs = 80\n",
    "batch_size = 32\n",
    "datagen_train_fine = ImageDataGenerator(rescale=1./255,\n",
    "                                shear_range=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                vertical_flip=True,\n",
    "                                rotation_range=90)\n",
    "\n",
    "generator_train_fine = datagen_train_fine.flow_from_directory(train_data_dir,  \n",
    "                                                target_size=(img_width, img_height),  \n",
    "                                                batch_size=batch_size,  \n",
    "                                                class_mode='categorical',  \n",
    "                                                shuffle=False)\n",
    "\n",
    "nb_train_samples_fine = len(generator_train_fine.filenames)  \n",
    "num_classes = len(generator_train_fine.class_indices)  \n",
    "\n",
    "train_data = generator_train_fine\n",
    "train_labels = generator_train_fine.classes  \n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "\n",
    "generator_validation_fine = datagen_train_fine.flow_from_directory(validation_data_dir,  \n",
    "                                                        target_size=(img_width, img_height),  \n",
    "                                                        batch_size=batch_size,  \n",
    "                                                        class_mode='categorical',  \n",
    "                                                        shuffle=False)  \n",
    "\n",
    "nb_validation_samples_fine = len(generator_validation_fine.filenames)\n",
    "\n",
    "validation_data = nb_validation_samples_fine\n",
    "validation_labels = generator_validation_fine.classes\n",
    "validation_labels = to_categorical(validation_labels, num_classes=num_classes)\n",
    "\n",
    "\n",
    "base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height,3))\n",
    "\n",
    "print(base_model.output_shape[1:])\n",
    "\n",
    "top_model = Sequential()  \n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))  \n",
    "top_model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  \n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "top_model.compile(optimizer=optimizers.RMSprop(lr=0.01, decay=0.1), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "model_fine = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "\n",
    "for layer in model_fine.layers[:15]:\n",
    "    print(layer.name)\n",
    "    layer.trainable = False\n",
    "model_fine.layers[15].name\n",
    "\n",
    "model_fine.compile(optimizer=optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model_fine.summary())\n",
    "\n",
    "model_fine = load_model('MRETZLAF_DAMAGED_OR_NOT_FINE.hdf5')\n",
    "\n",
    "checkpointer = ModelCheckpoint(monitor='val_acc', filepath='MRETZLAF_DAMAGED_OR_NOT_FINE.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "history_fine = model_fine.fit_generator(generator_train_fine,\n",
    "                                        steps_per_epoch=nb_train_samples_fine // batch_size,\n",
    "                                        epochs=epochs,\n",
    "                                        validation_data=generator_validation_fine,\n",
    "                                        validation_steps=nb_validation_samples_fine // batch_size,\n",
    "                                        verbose=1,\n",
    "                                        callbacks=[checkpointer])\n",
    "\n",
    "plt.plot(history_fine.history['loss'])\n",
    "plt.plot(history_fine.history['val_loss'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_fine.history['acc'])\n",
    "plt.plot(history_fine.history['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "\n",
    "**Accuracy**\n",
    "- Acc: 0,9784\n",
    "- Val Acc: 0,954\n",
    "![Accuracy](Fine_acc_red2.png)\n",
    "\n",
    "**Loss**\n",
    "- Loss: 0,0952\n",
    "- Val Loss: 0,1712\n",
    "![title](Fine_loss_red2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_fine = load_model('MRETZLAF_DAMAGED_OR_NOT_FINE.hdf5')\n",
    "\n",
    "test_data_dir = 'C:/data/car-damage/damaged_or_not/testing'\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "flow_test = test_datagen.flow_from_directory(test_data_dir,  target_size=(img_width, img_height),  batch_size=batch_size,class_mode='categorical')\n",
    "print(model_fine.metrics_names)\n",
    "model_fine.evaluate_generator(flow_test,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test Acc: 0,953135\n",
    "- Test Loss: 0,2075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Con la primer red se obtuvieron muy buenos resultados probando la red en un entorno de testing con datos reales. Se está trabajando para implementarla en producción aunque se debe probar con mas datos reales.\n",
    "\n",
    "La segunda red no tuvo tan buenos resultados ya que los daños pequeños como rayones o aboyaduras chicas muchas veces no son detectados correctamente. Se está trabajando en un nuevo modelo de detección de daño que permita clasificar el daño en 3 categorias: Leve - Moderado - Grave. Con este enfoque, se trata de obtener un resultado más productivo para el negocio ya que es más fácil detectar daños graves que daños leves y podría hacerse una clasificación automática de los siniestros graves sin necesidad de que intervenga un tasador, dejando el resto de los casos para ser validados por una persona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando la red (Web Service)\n",
    "\n",
    "Para utilizar la red 1, la disponibilizamos como webservices, recibiendo la url de una imagen a detectar y produciendo una salida json con la probabilidad de que sea un auto.\n",
    "\n",
    "![Title](webservice.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
